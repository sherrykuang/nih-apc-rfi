{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1690b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "compiled_df = pd.read_csv('nih.apc.csv')\n",
    "\n",
    "# Define file path to folder containing all uploaded PDFs\n",
    "file_path = r\"C:\\Users\\sherr\\OneDrive - University of Ottawa\\uOttawa MIS\\ROARA\\nih-apc-rfi\\files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227fe7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting in-text citations ###\n",
    "\n",
    "def extract_citations(text):\n",
    "\n",
    "    res = []\n",
    "\n",
    "    citation_patterns = [\n",
    "        \n",
    "    # APA in-text citation styles \n",
    "\n",
    "        # (Author, Year) OR (Author & Author, Year) OR (Author et al., Year, Page#))\n",
    "        r'\\([^)]+?,\\s*\\d{4}(?:[a-z])?(?:,\\s*p{1,2}\\.?\\s*\\d+(?:-\\d+)?)?\\)',\n",
    "\n",
    "        # Author (Year) OR Author et al. (Year) OR Author and Author (Year)\n",
    "        r'[A-Z][a-zA-Z\\-\\']+(?:(?:\\s+(?:and|&)\\s+[A-Z][a-zA-Z\\-\\']+)+|\\s+et\\s+al\\.)?\\s*\\(\\d{4}(?:[a-z])?\\)',\n",
    "\n",
    "        # Author et al. (Author, Year)\n",
    "        r'[A-Z][a-zA-Z\\-\\']+\\s+et\\s+al\\.\\s*\\([^)]+?,\\s*\\d{4}\\)',\n",
    "\n",
    "    # MLA in-text citation styles\n",
    "\n",
    "        # (Author Page#) OR (Author and Author Page#) OR (Author et al. Page#)\n",
    "        r'\\([A-Z][a-zA-Z\\-\\']+(?:\\s+and\\s+[A-Z][a-zA-Z\\-\\']+|\\s+et\\s+al\\.)?\\s+\\d+(?:-\\d+)?\\)',\n",
    "\n",
    "        # (\"Title\" Page#)\n",
    "        r'\\(\"[^\"]+\"\\s+\\d+(?:-\\d+)?\\)',\n",
    "\n",
    "        # Author (Page#)\n",
    "        r'\\([A-Z][a-zA-Z\\-\\']+\\s+\\d+(?:-\\d+)?\\)',\n",
    "    \n",
    "    # Chicago \n",
    "    \n",
    "        # (Author Year) OR (Author Year, Page#)\n",
    "        r'\\([A-Za-z\\s\\.\\-\\'&]+?\\s\\d{4}(?:,\\s\\d+(?:-\\d+)?)?\\)',\n",
    "\n",
    "        # (Year, Page#)\n",
    "        r'\\(\\d{4},\\s\\d+(?:-\\d+)?\\)',\n",
    "\n",
    "        # (Author, n.d.) OR (Author n.d., Page#)\n",
    "        r'\\([A-Za-z\\s\\.\\-\\']+,?\\sn\\.d\\.(?:,\\s\\d+(?:-\\d+)?)?\\)',\n",
    "\n",
    "    ]\n",
    "\n",
    "    blocklist = [\"option\", \"table\", \"figure\", \"box\"]\n",
    "\n",
    "    for pattern in citation_patterns:\n",
    "        \n",
    "        matches = re.findall(pattern, text)\n",
    "\n",
    "        for match in matches:\n",
    "\n",
    "            if isinstance(match, tuple):\n",
    "                citation_str = match[0]\n",
    "            else:\n",
    "                citation_str = match\n",
    "                \n",
    "            # remove newline characters \n",
    "            citation_str = citation_str.replace('\\n', '')\n",
    "        \n",
    "            if not any(word in citation_str.lower() for word in blocklist):\n",
    "                res.append(citation_str)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6dc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting DOIs, PMIDs, and PMCIDs ###\n",
    "\n",
    "def extract_identifiers(text):\n",
    "\n",
    "    doi_pattern = r'\\b(10\\.\\d{4,9}/[-._;()/:a-zA-Z0-9]+)\\b'\n",
    "    pmid_pattern = r'PMID:?\\s*(\\d{1,8})'\n",
    "    pmcid_pattern = r'\\bPMC\\d{1,8}\\b'\n",
    "\n",
    "    return re.findall(doi_pattern, text, re.IGNORECASE) + re.findall(pmid_pattern, text, re.IGNORECASE) + re.findall(pmcid_pattern, text, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50927d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting URLs and web domains ###\n",
    "\n",
    "def extract_urls(text): \n",
    "\n",
    "    # remove email addresses from text\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "    clean_text = re.sub(email_pattern, ' ', text)\n",
    "\n",
    "    # URL and web domain patterns \n",
    "    url_pattern = r'(https?://[^\\s]+)'\n",
    "    domain_pattern = r'\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\\b'\n",
    "\n",
    "    raw_urls = re.findall(url_pattern, clean_text)\n",
    "    raw_domains = re.findall(domain_pattern, clean_text)\n",
    "\n",
    "    # Clean raw URL and domain matches \n",
    "    clean_urls = {re.sub(r'[.,;)]$', '', u) for u in raw_urls}\n",
    "    clean_domains = {re.sub(r'[.,;)]$', '', d) for d in raw_domains}\n",
    "\n",
    "    # Remove duplicate domains that are part of extracted URLs\n",
    "    final_domains = []\n",
    "\n",
    "    for domain in clean_domains:\n",
    "        is_duplicate = False\n",
    "\n",
    "        for url in clean_urls:\n",
    "            if domain in url:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "            \n",
    "        if not is_duplicate:\n",
    "            final_domains.append(domain)\n",
    "\n",
    "    return list(clean_urls.union(final_domains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a8237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract sources from compiled dataframe ###\n",
    "\n",
    "compiled_res = []\n",
    "\n",
    "for index, row in compiled_df.iterrows():\n",
    "\n",
    "    record_id = row['Record.ID']\n",
    "    comment = row['Comment']\n",
    "    citations = extract_citations(comment)\n",
    "    links = extract_urls(comment)\n",
    "    identifiers = extract_identifiers(comment)\n",
    "\n",
    "    if (not citations) and (not links) and (not identifiers):\n",
    "        continue\n",
    "\n",
    "    compiled_res.append({'Record.ID': record_id, 'Citation': citations, 'Identifiers': identifiers, 'Links': links})\n",
    "\n",
    "compiled_df = pd.DataFrame(compiled_res)\n",
    "# print(compiled_df_df.head())\n",
    "\n",
    "compiled_df.to_csv('compiled_sources.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dacc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the sources defined above and also all hyperlinks from uploaded PDFs ###\n",
    "\n",
    "def extract_hyperlinks(file_path):\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    hyperlinks = set()\n",
    "\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                for link in page.get_links():\n",
    "                    if \"uri\" in link:\n",
    "                        uri = link[\"uri\"].strip()\n",
    "\n",
    "                        # Filter out mailto and tel addresses\n",
    "                        if not uri.lower().startswith((\"mailto:\", \"tel:\", \"whatsapp:\")):\n",
    "                            hyperlinks.add(uri)\n",
    "    except Exception:\n",
    "        return []\n",
    "    return list(hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract text from uploaded PDFs for regex extractions ###\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "\n",
    "    uploaded_texts = []\n",
    "\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                uploaded_texts.append(page.get_text())\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(uploaded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e6f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract sources and hyperlinks from uploaded PDFs and store in separate csv ### \n",
    "\n",
    "uploaded_res = []\n",
    "# output_csv = \"uploaded_sources.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "\n",
    "    for filename in os.listdir(file_path):\n",
    "\n",
    "        # PDF file name is exactly 2 OR 3 digits\n",
    "        if re.match(r'^\\d{2,3}\\.pdf$', filename, re.IGNORECASE):\n",
    "            \n",
    "            current_file_path = os.path.join(file_path, filename)\n",
    "            record_id = os.path.splitext(filename)[0]\n",
    "            \n",
    "            try:\n",
    "                # Extract Content\n",
    "                plain_text = extract_pdf_text(current_file_path)\n",
    "                embedded_links = extract_hyperlinks(current_file_path)\n",
    "\n",
    "                # Run Regex\n",
    "                citations = extract_citations(plain_text)\n",
    "                ids = extract_identifiers(plain_text)\n",
    "                text_links = extract_urls(plain_text)\n",
    "                    \n",
    "                # Combine links\n",
    "                all_links = list(set(embedded_links + text_links))\n",
    "                    \n",
    "                # Store results if data found\n",
    "                if citations or all_links or ids.get('DOIs') or ids.get('PMIDs') or ids.get('PMCIDs'):\n",
    "                    uploaded_res.append({\n",
    "                        'Record.ID': record_id,\n",
    "                        'Citation': citations,\n",
    "                        'Identifiers': ids,\n",
    "                        'Links': all_links\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "    if uploaded_res:\n",
    "        uploaded_df = pd.DataFrame(uploaded_res)\n",
    "\n",
    "    else:\n",
    "        print(\"No matches found in files\")\n",
    "else:\n",
    "    print(\"Folder path not found\")\n",
    "\n",
    "uploaded_sources = uploaded_df.to_csv('uploaded_sources.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge main sources and uploaded sources dataframes based on Record.ID and stores unique results ###\n",
    "\n",
    "combined_df = pd.concat([compiled_df, uploaded_df])\n",
    "\n",
    "combined_df['Record.ID'] = combined_df['Record.ID'].astype(str)\n",
    "\n",
    "agg_func = lambda source: list(set(sum(source, [])))\n",
    "\n",
    "final_df = combined_df.groupby(\"Record.ID\", as_index=False).agg({\n",
    "    \"Citation\": agg_func,\n",
    "    \"Identifiers\": agg_func,\n",
    "    \"Links\": agg_func\n",
    "})\n",
    "\n",
    "final_df.to_csv('all_sources.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
